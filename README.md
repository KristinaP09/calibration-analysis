# calibration-analysis

Python Implementation for Uncertainty Quantification in Binary Classification Models: A Comprehensive Analysis of Post-Hoc Calibration Methods

This repository contains a Python implementation for uncertainty quantification in binary classification models, focusing on post-hoc calibration methods. The implementation includes techniques such as Platt scaling and isotonic regression to improve the calibration of probabilistic predictions made by machine learning models.

### **Repository Overview**

This repository provides a comprehensive analysis of post-hoc calibration methods for binary classification models. The main features of the repository include:

- Implementation of Platt scaling and isotonic regression techniques.
- Evaluation metrics for assessing calibration performance, such as Brier score and reliability diagrams.
- Jupyter notebooks for interactive exploration and visualization of calibration methods.
- Integration with popular machine learning libraries like scikit-learn and TensorFlow.
- Example datasets for testing and benchmarking calibration methods.

### **Getting Started**
To get started with the repository, follow these steps:
1. Clone the repository:
   ```bash
   git clone
   git clone
   https://github.com/KristinaP09/calibration-analysis.git
    ```

2. Navigate to the project directory:
    ```bash
    cd calibration-analysis
    ```
3. Install the required dependencies:
    ```bash
   pip install -r requirements.txt
    ```
4. Explore the Jupyter notebooks for examples and usage instructions:
    ```bash
    jupyter notebook
     ```
5. Run the example scripts to see the calibration methods in action.
### **GitHub Repository**
All code, experimental scripts, and documentation are available at:
**Repository:** [https://github.com/KristinaP09/calibration-analysis](https://github.com/KristinaP09/calibration-analysis)
